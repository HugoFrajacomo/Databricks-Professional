{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca00ad27-dd7c-4ee5-9314-093179e92096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Definindo máximo de byter por Trigger\n",
    "\n",
    "Se você estiver ingerindo arquivos grandes que causam tempos longos de processamento em micro-batches ou problemas de memória, você pode usar a opção cloudFiles.maxBytesPerTrigger para controlar a quantidade máxima de dados processados em cada micro-batch. Isso melhora a estabilidade e mantém a duração dos batches mais previsível. Por exemplo, para limitar cada micro-batch a 1 GB de dados, você pode configurar seu stream da seguinte forma:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2df2c1b3-cb1f-4a59-a1ba-1c7731997882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    spark.readStream\n",
    "         .format(\"cloudFiles\")\n",
    "         .option(\"cloudFiles.format\", <source_format>)\n",
    "         .option(\"cloudFiles.maxBytesPerTrigger\", \"1g\") #Expressão que define o tamanho máximo de dados processados por trigger\n",
    "         .load(\"/path/to/files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c07d7df-d40a-44d9-84de-1b0ea29cd071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lidando com registros ruins\n",
    "\n",
    "Ao trabalhar com arquivos JSON ou CSV, você pode utilizar a opção badRecordsPath para capturar e isolar registros inválidos em um local separado para análise posterior. Registros com sintaxe malformada, como colchetes ausentes ou vírgulas extras, ou com incompatibilidades de esquema, como erros de tipo de dados ou campos ausentes, são redirecionados para o caminho especificado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07c9b6d0-1c30-40ae-b8b8-c30f71fd1220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    spark.readStream\n",
    "         .format(\"cloudFiles\")\n",
    "         .option(\"cloudFiles.format\", \"json\")\n",
    "         .option(\"badRecordsPath\", \"/path/to/quarantine\") #Expressão que define o caminho para armazenar os registros inválidos\n",
    "         .schema(\"id int, value double\")\n",
    "         .load(\"/path/to/files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4457ce01-66fb-4396-ae24-9f56ac8374f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filtro de tipagem de arquivos\n",
    "\n",
    "Para filtrar arquivos de entrada com base em um padrão específico, como por exemplo *.png, você pode utilizar a opção pathGlobFilter. Esta configuração permite que o seu processo de ingestão ignore arquivos irrelevantes no diretório de origem e processe apenas aqueles que atendem ao critério definido, otimizando o tempo de execução e o consumo de recursos do cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "798da53c-c0bc-4dd9-aa9f-03b333f90e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    spark.readStream\n",
    "         .format(\"cloudFiles\")\n",
    "         .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "         .option(\"pathGlobfilter\", \"*.png\") #Expressão que define o filtro de arquivos(*.png, *.jpg, *.gif...)\n",
    "         .load(\"/path/to/files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2131848-56c3-4a4a-b386-e7576c1b648a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evolução do Esquema (Schema)\n",
    "O Auto Loader detecta a adição de novas colunas em arquivos de entrada durante o processamento. Para controlar como essa mudança de esquema é gerenciada, você pode configurar a opção cloudFiles.schemaEvolutionMode. Essa funcionalidade permite que o seu pipeline se adapte automaticamente a mudanças nos dados de origem sem a necessidade de intervenção manual ou reinicialização do stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "696fce18-d51a-4a23-8a9b-b6e4cffb99aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    spark.readStream\n",
    "         .format(\"cloudFiles\")\n",
    "         .option(\"cloudFiles.format\", <source_format>) #Expressão que define o formato de dados\n",
    "         .option(\"cloudFiles.schemaEvolutionMode\", <mode>) #Expressão que define o modo de evolução do esquema\n",
    "         .load(\"/path/to/files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6643f65d-f95b-405d-8577-634e58b67c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Modos de evolução de esquema\n",
    "\n",
    "| Modo | Comportamento ao ler uma nova coluna |\n",
    "| :--- | :--- |\n",
    "| `addNewColumns` (padrão) | O fluxo falha. Novas colunas são adicionadas ao esquema. |\n",
    "| `rescue` | O esquema nunca evolui e o fluxo não falha devido a mudanças de esquema. Todas as novas colunas são registradas na coluna de dados recuperados (`rescued data column`). |\n",
    "| `failOnNewColumns` | O fluxo falha. O fluxo não reinicia a menos que o esquema fornecido seja atualizado ou o arquivo de dados problemático seja removido. |\n",
    "| `none` | Não evolui o esquema, novas colunas são ignoradas e os dados não são recuperados. O fluxo não falha devido a mudanças de esquema. |\n",
    "\n",
    "\n",
    "O modo padrão é o addNewColumns, portanto, quando o Auto Loader detecta uma nova coluna, o fluxo (stream) é interrompido com uma exceção do tipo UnknownFieldException. Antes que o seu fluxo lance esse erro, o Auto Loader atualiza o local do esquema com a versão mais recente, mesclando as novas colunas ao final do esquema. A próxima execução do fluxo ocorrerá com sucesso utilizando o esquema atualizado.\n",
    "\n",
    "É importante observar que o modo addNewColumns é o padrão quando um esquema não é fornecido, mas o modo none torna-se o padrão quando você fornece um esquema manualmente. Além disso, o uso de addNewColumns não é permitido quando o esquema do fluxo já foi explicitamente definido."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02.AutoLoader_Configuration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
